# -*- coding: utf-8 -*-
"""text_extraction_OCR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pcZ4qWVtw7aadrCbcEGL8_bORAH1tT7p
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
from matplotlib.pyplot import figure
import easyocr
# from PIL import Image

root_path = os.getcwd()
image_path = os.path.join(root_path,'1.jpg')
img = cv2.imread(image_path)
print(img.shape)
plt.imshow(img)

gray_img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
print(gray_img.shape)
plt.imshow(gray_img)

img_1 =cv2.cvtColor(img,cv2.COLOR_BGR2RGB)

thre_img = cv2.threshold(gray_img,50,100,8)[1]
resized_img = cv2.resize(thre_img, None, fx=3, fy=3, interpolation=cv2.INTER_CUBIC)
figure(figsize=(10,8),dpi=80)
print(thre_img.shape)
plt.imshow(thre_img)

# contours, _ = cv2.findContours(resized_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
# x, y, w, h = cv2.boundingRect(max(contours, key=cv2.contourArea))
# cropped = resized_img[y:y+h, x:x+w]
# figure(figsize=(8,8),dpi=80)
# plt.imshow(cropped)

# threshold_img = cv2.adaptiveThreshold(gray_img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)
# resized_img1 = cv2.resize(threshold_img, None, fx=5, fy=5, interpolation=cv2.INTER_LINEAR)

# kernel = np.ones((3,3), np.uint8)
# cleaned = cv2.morphologyEx(resized_img1, cv2.MORPH_OPEN, kernel, iterations=2)

# figure(figsize=(8,8),dpi=80)
# plt.imshow(cleaned)

edge = cv2.Canny(thre_img,150,255,None,3)
figure(figsize=(10,8),dpi=80)
plt.imshow(edge)

# sobel_x = cv2.Sobel(thre_img, cv2.CV_64F, 1, 0, ksize=3)
# sobel_y = cv2.Sobel(thre_img, cv2.CV_64F, 0, 1, ksize=3)
# sobel_edge = cv2.magnitude(sobel_x, sobel_y)
# plt.imshow(sobel_edge, cmap='gray')

# laplacian = cv2.Laplacian(thre_img, cv2.CV_64F)
# plt.imshow(laplacian, cmap='gray')

# scharr_x = cv2.Scharr(thre_img, cv2.CV_64F, 1, 0)
# scharr_y = cv2.Scharr(thre_img, cv2.CV_64F, 0, 1)
# scharr_edge = cv2.magnitude(scharr_x, scharr_y)
# plt.imshow(scharr_edge, cmap='gray')

import math

#lines = cv2.HoughLines(dst, 1, np.pi / 180, 150, None, 0, 0)

edge_rgb = cv2.cvtColor(edge, cv2.COLOR_GRAY2RGB)

rho = 1  # distance resolution in pixels of the Hough grid
theta = np.pi / 180  # angular resolution in radians of the Hough grid
threshold = 25 # minimum number of votes (intersections in Hough grid cell)
min_line_length =40 # minimum number of pixels making up a line
max_line_gap = 15  # maximum gap in pixels between connectable line segments
line_image = np.copy(thre_img) * 0  # creating a blank to draw lines on

# Run Hough on edge detected image
# Output "lines" is an array containing endpoints of detected line segments
lines = cv2.HoughLinesP(edge, rho, theta, threshold, np.array([]),
                    min_line_length, max_line_gap)
print("No of lines : ",len(lines))
print(lines)

def filter_lines(lines,min_length=300,angle_threshold=10):
  filtered_lines = []
  for line in lines:
    x1,y1,x2,y2 = line[0]

    length = np.sqrt((y2-y1)**2+(x2-x1)**2)
    if length < min_length:
      continue

    angle = np.arctan2((y2-y1),(x2-x1))*180/np.pi
    if abs(angle)<angle_threshold or abs(angle-180)<angle_threshold:
      filtered_lines.append([x1,y1,x2,y2])

  return np.array(filtered_lines)

filtered_lines = filter_lines(lines)

# for line in filtered_lines:
#   x1,y1,x2,y2 =line
#   cv2.line(edge_rgb,(x1,y1),(x2,y2),(0,255,0),2)#Green lines

for line in lines:
    x1, y1, x2, y2 = line[0]
    cv2.line(edge_rgb, (x1, y1), (x2, y2), (255, 0, 0), 1)#Blue lines

print("No of filtered_lines : ",len(filtered_lines))

plt.imshow(cv2.cvtColor(edge_rgb, cv2.COLOR_BGR2RGB))
plt.axis('off')
plt.show()

print(lines[0][0][0])

actual_lines = []
actual_points =[]
y_distance = 50

for line in filtered_lines:
    x1, y1, x2, y2 = line

    if len(actual_lines) == 0:
            #add first line point
            actual_lines.append(y1)
            actual_points.append([x1,y1,x2,y2])

    else:
            is_needAdd =True
            for actual_y in actual_lines:
                # first line and second line must be a least y_distance
                if(np.abs(actual_y-y1) < y_distance):
                    is_needAdd = False
                    break

            if is_needAdd:
                actual_lines.append(y1)
                actual_points.append([x1,y1,x2,y2])

print(actual_lines)
print(actual_points)

lines_total = [
    [41, 364, 430, 364],
    [40, 152, 430, 152],
    [40, 564, 427, 564],
    [38, 101, 426, 101],
    [56, 760, 398, 760],
    [46, 614, 425, 608],
    [40, 38, 423, 32],
    [56, 684, 394, 684]
]

lines_to_remove = [
    [46, 614, 425, 608],
    [56, 760, 398, 760],
    [56, 684, 394, 684]
]

total_filtered_lines = [line for line in lines_total if line not in lines_to_remove]

print(total_filtered_lines)

img_output = img.copy()
print(total_filtered_lines)
for x1,y1,x2,y2 in total_filtered_lines:
    cv2.line(img_output,(x1,y1),(x2,y2),(255,0,0),1)
figure(figsize=(10,8),dpi=80)
plt.imshow(img_output)

img1 = img_output.copy()

fist_part = total_filtered_lines[3]
print(fist_part)
first_img = img[:fist_part[3],fist_part[0]:fist_part[2],:]
print(first_img.shape)
plt.imshow(first_img)

img2 = img_output.copy()
second_part = total_filtered_lines[1]
print(second_part)
second_img = img[fist_part[3]:second_part[3],second_part[0]:second_part[2],:]
print(second_img.shape)
plt.imshow(second_img)

third_part = total_filtered_lines[0]
print(third_part)
third_img = img[second_part[3]:third_part[3],third_part[0]:third_part[2],:]
print(third_img.shape)
plt.imshow(third_img)

fourth_part = total_filtered_lines[2]
print(fourth_part)
fourth_img = img[third_part[3]:fourth_part[3],fourth_part[0]:fourth_part[2],:]
print(fourth_img.shape)
plt.imshow(fourth_img)


test_second_img = second_img.copy()
gray_image = cv2.cvtColor(test_second_img, cv2.COLOR_BGR2GRAY)
plt.imshow(gray_image)



reader = easyocr.Reader(['en'])

text_from_first_part = reader.readtext(first_img)

extracted_text = " ".join([text[1] for text in text_from_first_part])
print("Text from the first part:")
print(extracted_text)

text_from_second_part = reader.readtext(second_img)

extracted_text_2 = " ".join([text[1] for text in text_from_second_part])
print("Text from the second part:")
print(extracted_text_2)

text_from_third_part = reader.readtext(third_img)

extracted_text_3 = "/n".join([text[1] for text in text_from_third_part])
print("Text from the third part:")
print(extracted_text_3)

text_from_fourth_part = reader.readtext(fourth_img)

extracted_text_4 = " ".join([text[1] for text in text_from_fourth_part])
print("Text from the fourth part:")
print(extracted_text_4)